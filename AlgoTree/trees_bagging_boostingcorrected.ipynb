{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees, Bagging and Boosting\n",
    "\n",
    "This lab illustrates one learning algorithm used to build decision trees and two learning meta-algorithms, the _bagging_ and the _boosting_.\n",
    "\n",
    "<font color='red'>For pedagogical reasons, we code these models _almost_ from scratch, however these models are already implemented in `sklearn`. Outside of this lab, one should use the `sklearn` implementations.</font>\n",
    "\n",
    "In this lab, we work on the Boston housing data. Using this data set we want to build a model explaining the median value of owner-occupied homes. When facing a real data set, do not hesitate to do some plots first in order to get a better insight of the data.\n",
    "\n",
    "# Decision Trees\n",
    "A decision tree is a model that splits the input space recursively into regions $R_1,\\ldots,R_M$. These regions are a partition of the input space. Each region $R_m$ is associated to a predicted value $c_k$. For all the input $x\\in R_m$, the tree model $h$ predicts $c_m$. In the regression context, we have $h(x)=\\sum_{m=1}^M c_m {\\mathbb{1}}_{R_m}(x)$.\n",
    "\n",
    "Usually, each node of the tree model is associated to one condition on one explanatory variable. Considering a new example $x$, if it satisfies the condition at the node then it goes to the left branch else it goes to the right branch. This process is repeated till a leaf is reached. The value predicted is the one associated to this leaf. The Figure 1 shows a tree model and the associated partition.\n",
    "\n",
    "<img src=\"figure.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Building a tree model minimizing the error on the training set $S$ is a difficult task. Hence, a greedy recursive algorithm is used to build the tree model in a top-down fashion. First, the condition $\\mathrm{condition}$ associated to the root node is selected. Secondly, the left (resp. right) subtree is built by applying this algorithm on the examples of $S$ satisfying $\\mathrm{condition}$ (resp. $\\neg \\mathrm{condition}$). This algorithm stops when there are not enough examples to perform a split. This algorithm is described below:\n",
    "\n",
    "<img src=\"algo.png\" width=\"700\">\n",
    "\n",
    "The function ``chooseSplit`` chooses the \"best\" split $x_j\\leq t$ where $x_j$ is an explanatory variable and $t$ is the threshold of the condition. On the left leaf, we predict $c_1$ and on the right leaf we predict $c_2$. Here, the \"best\" is the one minimizing the following error on the training set:\n",
    "\\begin{equation}\n",
    "error(j,t)= \\underset{c_1}{min}~\\underset{\\substack{(x,y)\\in \\mathrm{data} \\\\ x_j\\leq t}}{\\sum} (y-c_1)^2 + \\underset{c_2}{min}~\\underset{\\substack{(x,y)\\in \\mathrm{data} \\\\ x_j > t}}{\\sum} (y-c_2)^2.\n",
    "\\end{equation}\n",
    "\n",
    "In order to control the size of the tree, ``chooseSplit`` only consider the splits that result in at least `nmin` examples on both side of the split. For this reason, sometimes, no split can be performed.\n",
    "\n",
    "\n",
    "<font color='red'>**Q1:**</font> When we select the \"best\" split, why is the number of couples $(j,t)$ to compare is finite?\n",
    "\n",
    "<font color='red'>**Q2:**</font> Assuming the couple $(j,t)$ is selected, what is the value of $c_1$ and $c_2$?\n",
    "\n",
    "<font color='red'>**Q3:**</font> The function `chooseSplit` returns the best `(j,t)` if a split is possible, `None` otherwise. Using this function, implement the function `buildTree(X, y, nmin)` described by the Algorithm above. `nmin` is used to control the growth of the tree. It is minimum number of examples in each leaf. \n",
    "\n",
    "<font color='green'>**Hints for Q3:**</font> As a reminder, considering a numpy array `a`, if `condition` is boolean numpy array with a shape similar to `a` then `a[condition]` is a numpy array containing the elements of `a` for which `condition` is true. In addition, one can use `np.logical_not`, considering a numpy boolean array `a`, `np.logical_not(a)` applies the logical operator \"not\" to each element of `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "\n",
    "def rmse(ypred, y):\n",
    "    return np.sqrt(np.mean((ypred - y) ** 2))\n",
    "\n",
    "def chooseSplit(X, y, nmin):\n",
    "    def sse(y):\n",
    "        e = y - np.mean(y)\n",
    "        return np.sum(e * e)\n",
    "    def error(j, t):\n",
    "        sel = X[:, j] <= t\n",
    "        nsel = np.logical_not(sel)\n",
    "        if np.sum(nsel) >= nmin and np.sum(sel) >= nmin:\n",
    "            s = sse(y[sel]) + sse(y[nsel])\n",
    "        else:\n",
    "            s = float('inf')\n",
    "        return (s, (j, t))\n",
    "    totest = [(j, t) for j in range(X.shape[1]) for t in X[:, j]]\n",
    "    s, (j, t) = min([error(*x) for x in totest])\n",
    "    if s == float('inf'):\n",
    "        return None\n",
    "    else:\n",
    "        return (j, t)\n",
    "    \n",
    "class Leaf:\n",
    "    '''Class storing the Leaf'''\n",
    "    def __init__(self,meany):\n",
    "        self.meany = meany\n",
    "    def __repr__(self):\n",
    "        return \"Leaf({:.2f})\".format(self.meany)\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return self.meany\n",
    "        else:\n",
    "            return np.repeat(self.meany,X.shape[0])\n",
    "    def count_leaves(self):\n",
    "        return 1\n",
    "    \n",
    "class Node:\n",
    "    '''Class storing the Node'''\n",
    "    def __init__(self, split, left, right):\n",
    "        (j, t) = split\n",
    "        self.j = j\n",
    "        self.t = t\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    def __repr__(self):\n",
    "        return \"Node(j={0.j},t={0.t:.2f},left={0.left},right={0.right})\".format(self)\n",
    "    def predict(self,X):\n",
    "        def predictone(node, Xi):\n",
    "            if isinstance(node, Leaf):\n",
    "                return node.predict(Xi)\n",
    "            elif Xi[node.j] <= node.t:\n",
    "                return predictone(node.left, Xi)\n",
    "            else:\n",
    "                return predictone(node.right, Xi)\n",
    "        return np.array([predictone(self,Xi)  for Xi in X])\n",
    "    def count_leaves(self):\n",
    "        return self.left.count_leaves()+self.right.count_leaves()\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"BostonHousing.csv\")\n",
    "    y = df[\"medv\"].to_numpy()\n",
    "    X = df.drop(columns=[\"medv\"]).to_numpy()\n",
    "    return X,y\n",
    "    \n",
    "def main(nmin=1):\n",
    "    # returns (X, y) two numpy arrays corresponding to the Boston Housing median value prediction problem\n",
    "    X, y = load_data()#sklearn.datasets.load_boston(return_X_y=True)\n",
    "    Xts, Xvs, yts, yvs = sklearn.model_selection.train_test_split(X, y, random_state=42,train_size=350)\n",
    "    m = buildTree(Xts, yts, nmin)    \n",
    "    print(m)\n",
    "    def rmse(ypred, y):\n",
    "        return np.sqrt(np.mean((ypred - y)**2))\n",
    "    print(\"nmin:\", nmin)\n",
    "    print(\"number of leaves:\",m.count_leaves())\n",
    "    print(\"RMSE ON TRAINING SET:\", rmse(m.predict(Xts), yts))\n",
    "    print(\"RMSE ON VALIDATION SET:\", rmse(m.predict(Xvs), yvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "source": [
    "<font color='blue'>**A1:**</font> Let us assume we have $p$ variables and $n$ examples, we have $p$ possible values for $j$ and $n-1$ values for $t$. To understand that we only have $n-1$ values for $t$, let us imagine that we sort the values $x_{(1),j}<\\ldots<x_{(n),j}$; every $t\\in [x_{(i),j};x_{(i+1),j}[$ leads to the same partition of the examples, with the examples $(1)<\\ldots<(i)$ on the left side and the examples $(i+1)<\\ldots<(n)$ on the right side. Thus we obtain $(n-1)p$ possible split.\n",
    "\n",
    "<font color='blue'>**A2:**</font> $c_1=\\mathrm{mean}(y\\in \\mathrm{left\\_node})$ and $c_2=\\mathrm{mean}(y\\in \\mathrm{right\\_node})$ where  $\\mathrm{left\\_node}=\\lbrace y\\vert (x,y) \\in \\mathrm{data}~/~x_j\\leq t \\rbrace$  and $\\mathrm{right\\_node}=\\lbrace y\\vert (x,y) \\in \\mathrm{data}~/~x_j > t \\rbrace$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(j=5,t=6.94,left=Node(j=12,t=14.37,left=Node(j=7,t=1.36,left=Node(j=0,t=8.27,left=Node(j=0,t=4.90,left=Leaf(50.00),right=Node(j=0,t=5.67,left=Leaf(50.00),right=Leaf(50.00))),right=Leaf(27.90)),right=Node(j=5,t=6.54,left=Node(j=5,t=6.03,left=Node(j=5,t=6.02,left=Node(j=0,t=13.52,left=Node(j=1,t=21.00,left=Node(j=2,t=2.46,left=Leaf(26.40),right=Node(j=4,t=0.72,left=Node(j=10,t=17.40,left=Node(j=0,t=2.24,left=Node(j=4,t=0.46,left=Node(j=5,t=5.96,left=Node(j=0,t=0.07,left=Leaf(19.30),right=Leaf(19.70)),right=Node(j=0,t=0.05,left=Leaf(23.40),right=Leaf(21.70))),right=Node(j=11,t=378.35,left=Node(j=0,t=0.54,left=Leaf(24.30),right=Leaf(23.80)),right=Node(j=0,t=0.05,left=Leaf(23.30),right=Node(j=1,t=0.00,left=Node(j=0,t=0.08,left=Leaf(22.60),right=Leaf(22.70)),right=Node(j=0,t=0.09,left=Leaf(22.90),right=Leaf(22.80)))))),right=Leaf(19.10)),right=Node(j=10,t=18.40,left=Node(j=5,t=5.71,left=Node(j=0,t=0.25,left=Leaf(16.20),right=Leaf(16.10)),right=Node(j=0,t=0.14,left=Leaf(19.30),right=Leaf(20.30))),right=Node(j=2,t=5.19,left=Node(j=2,t=4.39,left=Leaf(17.50),right=Node(j=5,t=5.97,left=Node(j=0,t=0.03,left=Leaf(18.50),right=Leaf(18.70)),right=Leaf(19.00))),right=Node(j=6,t=42.60,left=Node(j=5,t=5.88,left=Node(j=6,t=40.30,left=Node(j=0,t=0.06,left=Leaf(22.00),right=Leaf(21.80)),right=Node(j=0,t=0.30,left=Node(j=0,t=0.08,left=Leaf(21.00),right=Leaf(21.10)),right=Leaf(20.60))),right=Node(j=0,t=0.16,left=Node(j=0,t=0.14,left=Leaf(22.60),right=Leaf(21.70)),right=Node(j=0,t=0.18,left=Leaf(24.70),right=Leaf(24.50)))),right=Node(j=5,t=4.96,left=Node(j=0,t=3.70,left=Leaf(21.90),right=Leaf(23.10)),right=Node(j=12,t=14.27,left=Node(j=0,t=0.06,left=Leaf(18.90),right=Node(j=5,t=6.00,left=Node(j=6,t=87.60,left=Node(j=0,t=3.16,left=Node(j=7,t=3.38,left=Node(j=0,t=0.07,left=Leaf(20.30),right=Node(j=0,t=0.11,left=Node(j=0,t=0.10,left=Leaf(20.00),right=Leaf(20.10)),right=Leaf(19.90))),right=Node(j=0,t=0.09,left=Leaf(20.70),right=Leaf(20.40))),right=Node(j=0,t=4.84,left=Leaf(20.60),right=Leaf(20.80))),right=Leaf(19.60)),right=Leaf(21.20))),right=Leaf(22.00))))))),right=Node(j=0,t=1.13,left=Leaf(15.30),right=Leaf(19.60)))),right=Node(j=2,t=5.13,left=Node(j=5,t=5.88,left=Node(j=0,t=0.04,left=Leaf(18.20),right=Node(j=0,t=0.07,left=Leaf(18.60),right=Leaf(18.70))),right=Leaf(19.60)),right=Leaf(17.10))),right=Leaf(15.00)),right=Leaf(11.90)),right=Node(j=10,t=20.20,left=Node(j=9,t=403.00,left=Node(j=9,t=193.00,left=Leaf(29.60),right=Node(j=12,t=11.10,left=Node(j=11,t=385.81,left=Node(j=7,t=7.83,left=Node(j=11,t=377.07,left=Node(j=3,t=0.00,left=Node(j=4,t=0.60,left=Node(j=0,t=2.30,left=Node(j=5,t=6.39,left=Node(j=5,t=6.07,left=Leaf(24.30),right=Node(j=5,t=6.09,left=Leaf(24.00),right=Node(j=0,t=0.13,left=Leaf(23.70),right=Leaf(23.80)))),right=Node(j=0,t=0.16,left=Leaf(24.50),right=Leaf(24.80))),right=Leaf(25.00)),right=Leaf(23.30)),right=Leaf(27.00)),right=Node(j=0,t=0.09,left=Leaf(26.40),right=Leaf(28.10))),right=Leaf(20.50)),right=Node(j=6,t=21.10,left=Node(j=4,t=0.44,left=Node(j=0,t=0.04,left=Leaf(23.50),right=Node(j=0,t=0.08,left=Leaf(24.10),right=Leaf(23.90))),right=Node(j=0,t=0.05,left=Leaf(25.00),right=Leaf(24.70))),right=Node(j=10,t=18.50,left=Node(j=9,t=243.00,left=Node(j=1,t=0.00,left=Node(j=0,t=0.03,left=Leaf(21.60),right=Leaf(21.20)),right=Leaf(20.50)),right=Node(j=11,t=393.55,left=Node(j=0,t=0.05,left=Node(j=0,t=0.04,left=Leaf(22.00),right=Leaf(21.90)),right=Leaf(21.60)),right=Node(j=12,t=7.43,left=Node(j=2,t=4.05,left=Node(j=0,t=0.03,left=Leaf(24.10),right=Leaf(24.60)),right=Node(j=0,t=0.04,left=Leaf(23.20),right=Leaf(22.10))),right=Node(j=2,t=5.32,left=Node(j=0,t=0.01,left=Leaf(22.00),right=Node(j=0,t=0.05,left=Leaf(22.30),right=Leaf(22.20))),right=Node(j=0,t=0.04,left=Leaf(22.90),right=Node(j=0,t=0.11,left=Leaf(23.00),right=Leaf(23.10))))))),right=Node(j=12,t=7.51,left=Node(j=0,t=0.11,left=Node(j=2,t=4.86,left=Node(j=0,t=0.04,left=Leaf(22.90),right=Leaf(22.90)),right=Node(j=0,t=0.05,left=Leaf(22.20),right=Leaf(22.20))),right=Node(j=0,t=0.15,left=Leaf(23.30),right=Node(j=0,t=0.18,left=Leaf(23.10),right=Leaf(23.00)))),right=Node(j=0,t=0.20,left=Node(j=0,t=0.11,left=Leaf(24.40),right=Leaf(25.00)),right=Node(j=0,t=0.23,left=Leaf(24.40),right=Leaf(24.30))))))),right=Node(j=5,t=6.23,left=Node(j=0,t=0.11,left=Node(j=0,t=0.02,left=Leaf(20.10),right=Leaf(20.10)),right=Node(j=5,t=6.08,left=Leaf(22.20),right=Node(j=5,t=6.11,left=Leaf(21.00),right=Node(j=0,t=0.12,left=Leaf(21.40),right=Leaf(21.50))))),right=Node(j=0,t=0.10,left=Leaf(22.20),right=Leaf(22.30))))),right=Node(j=0,t=0.14,left=Node(j=0,t=0.02,left=Leaf(16.50),right=Node(j=0,t=0.05,left=Leaf(19.80),right=Leaf(18.50))),right=Node(j=12,t=7.79,left=Leaf(25.00),right=Node(j=6,t=81.30,left=Node(j=0,t=5.82,left=Node(j=4,t=0.58,left=Node(j=0,t=3.57,left=Leaf(23.20),right=Leaf(23.00)),right=Leaf(22.60)),right=Leaf(21.40)),right=Node(j=3,t=0.00,left=Node(j=0,t=6.65,left=Node(j=0,t=5.82,left=Node(j=0,t=3.84,left=Leaf(19.90),right=Leaf(20.20)),right=Leaf(19.50)),right=Leaf(21.40)),right=Node(j=0,t=3.85,left=Leaf(21.70),right=Leaf(22.70))))))),right=Node(j=0,t=0.13,left=Node(j=6,t=54.40,left=Leaf(21.70),right=Node(j=5,t=6.13,left=Node(j=0,t=0.05,left=Leaf(20.60),right=Leaf(20.40)),right=Leaf(19.80))),right=Node(j=11,t=387.94,left=Node(j=0,t=0.23,left=Leaf(18.60),right=Node(j=0,t=0.64,left=Leaf(18.20),right=Leaf(18.40))),right=Node(j=0,t=0.21,left=Leaf(19.30),right=Leaf(19.20)))))),right=Node(j=12,t=5.68,left=Node(j=10,t=17.80,left=Node(j=11,t=393.37,left=Node(j=5,t=6.73,left=Node(j=1,t=0.00,left=Leaf(29.40),right=Node(j=0,t=0.02,left=Leaf(30.10),right=Leaf(29.80))),right=Node(j=0,t=0.03,left=Node(j=0,t=0.02,left=Leaf(31.10),right=Leaf(31.20)),right=Leaf(30.50))),right=Node(j=0,t=0.10,left=Node(j=0,t=0.06,left=Leaf(33.10),right=Node(j=0,t=0.10,left=Leaf(32.00),right=Leaf(32.50))),right=Leaf(35.10))),right=Node(j=0,t=0.13,left=Node(j=0,t=0.04,left=Node(j=0,t=0.04,left=Leaf(27.90),right=Leaf(28.00)),right=Leaf(26.60)),right=Leaf(22.80))),right=Node(j=7,t=3.79,left=Node(j=2,t=8.56,left=Node(j=12,t=7.60,left=Node(j=1,t=20.00,left=Node(j=2,t=2.89,left=Leaf(28.40),right=Node(j=0,t=0.07,left=Leaf(29.90),right=Node(j=0,t=0.61,left=Leaf(30.10),right=Leaf(30.10)))),right=Leaf(32.00)),right=Node(j=0,t=0.05,left=Leaf(28.40),right=Node(j=0,t=0.15,left=Leaf(27.50),right=Leaf(27.50)))),right=Node(j=0,t=0.07,left=Leaf(28.70),right=Node(j=0,t=0.11,left=Node(j=0,t=0.10,left=Leaf(22.80),right=Leaf(22.00)),right=Leaf(23.80)))),right=Node(j=12,t=9.54,left=Node(j=1,t=22.00,left=Node(j=11,t=393.74,left=Node(j=0,t=0.19,left=Node(j=0,t=0.05,left=Leaf(27.10),right=Node(j=0,t=0.06,left=Leaf(26.60),right=Leaf(26.20))),right=Leaf(25.10)),right=Leaf(28.60)),right=Node(j=7,t=6.19,left=Node(j=2,t=4.93,left=Node(j=0,t=0.04,left=Node(j=0,t=0.03,left=Leaf(23.90),right=Leaf(23.90)),right=Leaf(23.30)),right=Leaf(22.00)),right=Node(j=1,t=40.00,left=Leaf(26.60),right=Node(j=0,t=0.02,left=Leaf(24.50),right=Node(j=0,t=0.04,left=Leaf(24.80),right=Leaf(24.80)))))),right=Node(j=0,t=0.11,left=Leaf(22.00),right=Leaf(21.00))))))),right=Node(j=7,t=2.07,left=Node(j=0,t=6.29,left=Node(j=5,t=5.40,left=Node(j=0,t=0.21,left=Leaf(8.10),right=Node(j=0,t=2.37,left=Node(j=0,t=1.63,left=Leaf(14.40),right=Leaf(14.60)),right=Node(j=5,t=4.90,left=Leaf(11.80),right=Node(j=0,t=2.45,left=Leaf(13.10),right=Leaf(13.40))))),right=Node(j=6,t=88.40,left=Leaf(21.40),right=Node(j=12,t=17.19,left=Node(j=5,t=5.69,left=Node(j=0,t=0.26,left=Leaf(16.20),right=Leaf(15.60)),right=Node(j=5,t=5.94,left=Node(j=0,t=0.32,left=Leaf(17.40),right=Leaf(16.80)),right=Leaf(18.00))),right=Node(j=5,t=6.34,left=Node(j=6,t=97.90,left=Node(j=6,t=96.40,left=Node(j=10,t=20.20,left=Node(j=0,t=2.73,left=Node(j=0,t=0.15,left=Leaf(15.20),right=Leaf(15.40)),right=Leaf(14.90)),right=Leaf(14.30)),right=Node(j=0,t=0.15,left=Leaf(17.30),right=Leaf(17.80))),right=Node(j=0,t=2.38,left=Node(j=0,t=0.25,left=Leaf(13.30),right=Leaf(13.80)),right=Leaf(15.60))),right=Leaf(12.50))))),right=Node(j=4,t=0.67,left=Node(j=0,t=23.65,left=Node(j=5,t=6.22,left=Node(j=7,t=1.17,left=Node(j=0,t=11.11,left=Leaf(13.80),right=Leaf(13.80)),right=Node(j=5,t=5.57,left=Node(j=0,t=8.79,left=Leaf(11.70),right=Leaf(11.90)),right=Node(j=0,t=12.25,left=Leaf(10.20),right=Leaf(10.20)))),right=Node(j=6,t=93.30,left=Node(j=0,t=8.49,left=Leaf(14.50),right=Leaf(13.90)),right=Node(j=0,t=9.82,left=Leaf(13.30),right=Leaf(13.10)))),right=Leaf(16.30)),right=Node(j=12,t=18.85,left=Node(j=4,t=0.69,left=Node(j=0,t=13.36,left=Node(j=0,t=8.64,left=Leaf(13.80),right=Leaf(12.70)),right=Leaf(10.90)),right=Node(j=0,t=9.97,left=Leaf(15.40),right=Leaf(16.70))),right=Node(j=0,t=9.60,left=Node(j=12,t=19.92,left=Leaf(8.50),right=Node(j=7,t=1.94,left=Node(j=0,t=9.19,left=Node(j=7,t=1.53,left=Leaf(12.30),right=Node(j=0,t=7.37,left=Leaf(11.00),right=Node(j=0,t=8.15,left=Leaf(11.50),right=Leaf(11.30)))),right=Node(j=0,t=9.39,left=Leaf(12.80),right=Leaf(12.10))),right=Leaf(9.50))),right=Node(j=4,t=0.69,left=Node(j=0,t=24.80,left=Node(j=0,t=9.92,left=Leaf(6.30),right=Node(j=5,t=5.90,left=Node(j=0,t=15.86,left=Leaf(8.30),right=Leaf(8.30)),right=Node(j=0,t=10.83,left=Leaf(7.50),right=Node(j=0,t=14.24,left=Leaf(7.20),right=Leaf(7.20))))),right=Node(j=0,t=25.05,left=Leaf(5.60),right=Leaf(5.00))),right=Node(j=12,t=28.28,left=Node(j=5,t=6.46,left=Node(j=0,t=10.67,left=Leaf(11.80),right=Node(j=0,t=22.05,left=Leaf(10.50),right=Leaf(10.50))),right=Leaf(8.40)),right=Node(j=11,t=285.83,left=Node(j=0,t=13.68,left=Leaf(8.40),right=Leaf(8.80)),right=Node(j=0,t=16.81,left=Leaf(7.20),right=Leaf(7.40))))))))),right=Node(j=11,t=306.38,left=Node(j=12,t=17.31,left=Node(j=0,t=3.77,left=Node(j=0,t=0.98,left=Node(j=0,t=0.96,left=Leaf(14.80),right=Leaf(15.60)),right=Node(j=0,t=1.21,left=Leaf(17.40),right=Leaf(19.00))),right=Node(j=0,t=7.75,left=Leaf(14.90),right=Leaf(13.50))),right=Node(j=11,t=81.33,left=Node(j=11,t=0.32,left=Leaf(13.40),right=Node(j=0,t=4.75,left=Leaf(14.10),right=Leaf(14.10))),right=Node(j=6,t=97.40,left=Node(j=12,t=23.27,left=Node(j=0,t=1.61,left=Leaf(13.50),right=Leaf(13.40)),right=Node(j=0,t=1.39,left=Leaf(13.20),right=Leaf(13.30))),right=Leaf(13.00)))),right=Node(j=6,t=84.40,left=Node(j=11,t=393.29,left=Node(j=0,t=5.29,left=Node(j=0,t=0.13,left=Node(j=0,t=0.09,left=Leaf(21.70),right=Leaf(20.40)),right=Node(j=7,t=3.59,left=Node(j=4,t=0.49,left=Leaf(23.70),right=Node(j=0,t=0.18,left=Leaf(23.10),right=Leaf(23.20))),right=Node(j=0,t=0.22,left=Leaf(22.40),right=Leaf(22.50)))),right=Node(j=0,t=5.69,left=Leaf(19.10),right=Leaf(19.10))),right=Node(j=5,t=5.57,left=Leaf(17.50),right=Node(j=2,t=6.91,left=Node(j=0,t=0.01,left=Leaf(18.90),right=Leaf(19.40)),right=Node(j=6,t=58.00,left=Node(j=0,t=0.14,left=Leaf(20.30),right=Leaf(20.10)),right=Node(j=0,t=0.25,left=Leaf(19.80),right=Leaf(20.00)))))),right=Node(j=0,t=0.17,left=Node(j=1,t=0.00,left=Node(j=0,t=0.09,left=Leaf(20.50),right=Node(j=0,t=0.12,left=Leaf(18.80),right=Node(j=0,t=0.13,left=Leaf(19.50),right=Leaf(19.50)))),right=Leaf(27.10)),right=Node(j=6,t=91.30,left=Node(j=6,t=90.30,left=Node(j=2,t=8.14,left=Node(j=0,t=0.23,left=Leaf(16.60),right=Leaf(16.60)),right=Node(j=0,t=0.17,left=Leaf(18.30),right=Leaf(17.70))),right=Node(j=0,t=0.26,left=Leaf(19.40),right=Leaf(21.70))),right=Node(j=12,t=17.60,left=Node(j=0,t=0.17,left=Leaf(16.00),right=Node(j=0,t=0.56,left=Node(j=0,t=0.33,left=Leaf(18.40),right=Leaf(18.10)),right=Leaf(17.80))),right=Node(j=5,t=5.57,left=Node(j=0,t=0.25,left=Leaf(14.40),right=Leaf(13.60)),right=Node(j=0,t=4.87,left=Node(j=7,t=6.08,left=Node(j=0,t=0.21,left=Leaf(16.50),right=Leaf(16.70)),right=Leaf(15.00)),right=Node(j=0,t=5.44,left=Leaf(15.20),right=Leaf(14.60))))))))))),right=Node(j=5,t=7.42,left=Node(j=0,t=6.54,left=Node(j=7,t=1.88,left=Node(j=0,t=1.22,left=Leaf(41.30),right=Leaf(50.00)),right=Node(j=2,t=6.41,left=Node(j=4,t=0.49,left=Node(j=7,t=6.48,left=Node(j=5,t=7.24,left=Node(j=0,t=0.09,left=Node(j=0,t=0.03,left=Node(j=0,t=0.03,left=Leaf(33.40),right=Leaf(34.90)),right=Node(j=12,t=5.10,left=Node(j=5,t=7.15,left=Node(j=0,t=0.08,left=Node(j=0,t=0.06,left=Leaf(37.20),right=Leaf(37.30)),right=Leaf(37.00)),right=Leaf(36.40)),right=Node(j=0,t=0.07,left=Node(j=0,t=0.06,left=Leaf(36.10),right=Leaf(36.20)),right=Leaf(34.90)))),right=Leaf(33.10)),right=Node(j=2,t=2.18,left=Leaf(33.40),right=Node(j=0,t=0.07,left=Leaf(33.20),right=Leaf(33.20)))),right=Node(j=0,t=0.04,left=Node(j=0,t=0.02,left=Node(j=0,t=0.01,left=Leaf(32.20),right=Node(j=0,t=0.01,left=Leaf(32.70),right=Node(j=0,t=0.02,left=Leaf(32.90),right=Leaf(33.00)))),right=Node(j=0,t=0.04,left=Node(j=0,t=0.03,left=Leaf(34.90),right=Leaf(34.60)),right=Leaf(33.30))),right=Node(j=0,t=0.05,left=Leaf(30.30),right=Node(j=0,t=0.06,left=Leaf(29.00),right=Leaf(29.60))))),right=Node(j=0,t=0.36,left=Node(j=7,t=3.41,left=Node(j=0,t=0.05,left=Leaf(28.70),right=Leaf(26.70)),right=Leaf(23.60)),right=Node(j=7,t=1.93,left=Node(j=0,t=0.55,left=Leaf(36.50),right=Leaf(36.00)),right=Node(j=0,t=0.54,left=Node(j=0,t=0.51,left=Node(j=0,t=0.46,left=Leaf(31.70),right=Leaf(31.50)),right=Leaf(33.80)),right=Node(j=0,t=0.79,left=Leaf(30.70),right=Leaf(31.00)))))),right=Node(j=0,t=0.06,left=Leaf(23.90),right=Leaf(25.00)))),right=Node(j=0,t=8.25,left=Leaf(17.80),right=Leaf(10.40))),right=Node(j=0,t=2.01,left=Node(j=7,t=3.20,left=Node(j=0,t=0.53,left=Node(j=0,t=0.52,left=Node(j=2,t=3.97,left=Node(j=0,t=0.06,left=Leaf(50.00),right=Leaf(48.80)),right=Leaf(44.80)),right=Leaf(43.10)),right=Node(j=0,t=0.58,left=Leaf(50.00),right=Node(j=0,t=0.61,left=Leaf(50.00),right=Node(j=0,t=1.46,left=Leaf(50.00),right=Node(j=0,t=1.52,left=Leaf(50.00),right=Node(j=0,t=1.83,left=Leaf(50.00),right=Leaf(50.00))))))),right=Node(j=12,t=3.95,left=Node(j=0,t=0.33,left=Node(j=5,t=7.82,left=Node(j=8,t=3.00,left=Node(j=2,t=2.03,left=Leaf(42.30),right=Node(j=0,t=0.02,left=Leaf(44.00),right=Leaf(43.80))),right=Node(j=0,t=0.06,left=Leaf(46.00),right=Leaf(46.70))),right=Node(j=0,t=0.02,left=Leaf(50.00),right=Leaf(48.30))),right=Node(j=5,t=8.04,left=Leaf(37.60),right=Node(j=0,t=0.37,left=Leaf(42.80),right=Leaf(41.70)))),right=Node(j=0,t=0.12,left=Leaf(38.70),right=Leaf(35.20)))),right=Leaf(21.90))))\n",
      "nmin: 1\n",
      "number of leaves: 350\n",
      "RMSE ON TRAINING SET: 0.0\n",
      "RMSE ON VALIDATION SET: 4.159889359968543\n"
     ]
    }
   ],
   "source": [
    "def buildTree(X, y, nmin=1):\n",
    "        split = chooseSplit(X, y, nmin)\n",
    "        if split is None:\n",
    "            return Leaf(np.mean(y))\n",
    "        else:\n",
    "            (j, t) = split\n",
    "            sel = X[:,j] <= t\n",
    "            nsel = np.logical_not(sel)\n",
    "            left = buildTree(X[sel],y[sel],nmin)\n",
    "            right = buildTree(X[nsel],y[nsel],nmin)\n",
    "            return Node(split, left, right)\n",
    "        \n",
    "main(1)\n",
    "#main(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Q4:**</font> Using `nmin=1`, why the RMSE on the training set is 0 ? Looking at the RMSE on  the validation set, test with `nmin=4`, why the RMSE is reduced ?\n",
    "\n",
    "<font color='red'>**Q5:**</font> Rescaling the explanatory variables is an important pre-processing for some methods like neural network or penalized linear regression like Ridge for instance. For the regression tree model seen here, will it change a thing to scale the explanatory variables ? More generally, which property makes this transformation useless ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "source": [
    "<font color='blue'>**A4:**</font> If no examples are the same, with `nmin=1`, the tree is grown till each leaf contains only one example. With `nmin=4`, the model is more smooth, it does not overfit the training set anymore.\n",
    "\n",
    "<font color='blue'>**A5:**</font> No it will not change a thing. A strictly monotone transformation applied to an explanatory variable will not change the possible partition that can be obtained with the considered $x_j\\leq t$ splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex \n",
    "\n",
    "@article{breiman1996bagging,\n",
    "  title={Bagging predictors},\n",
    "  author={Breiman, Leo},\n",
    "  journal={Machine learning},\n",
    "  volume={24},\n",
    "  number={2},\n",
    "  pages={123--140},\n",
    "  year={1996},\n",
    "  publisher={Springer}\n",
    "}\n",
    "@article{breiman2001random,\n",
    "  title={Random forests},\n",
    "  author={Breiman, Leo},\n",
    "  journal={Machine learning},\n",
    "  volume={45},\n",
    "  number={1},\n",
    "  pages={5--32},\n",
    "  year={2001},\n",
    "  publisher={Springer}\n",
    "}\n",
    "\n",
    "-->\n",
    "# Bagging and Random Forest\n",
    "The _bagging_ is a learning meta-algorithm proposed in [(Bagging predictors, L. Breiman, Machine Learning 1996)](https://www.stat.berkeley.edu/~breiman/bagging.pdf). Considering an unstable learning algorithm $\\mathcal{A}$ (low bias and high variance), the _bagging_ computes the mean of several models built by $\\mathcal{A}$. The new learning algorithm obtained through the bagging algorithm have a variance inferior to the variance of $\\mathcal{A}$. This variance reduction reduces the prediction error.\n",
    "\n",
    "The _random forest_ is a learning algorithm proposed in [(Random forests, L. Breiman, Machine Learning 2001)](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf). It uses the _bagging_ with a learning algorithm that builds a tree model. This tree model is randomly distorted. With this, the estimated mean is computed over very different tree models. This reduces the variance of this estimated mean.\n",
    "## Bagging\n",
    "\n",
    "Let us consider a fixed $x_0$ and $Y_0=(Y|X=x_0)$, using a learning algorithm $\\mathcal{A}$, the _bagging_ takes advantage of the inequation below:\n",
    "$${\\mathbb{E}}_{Y_0,S}[(Y_0-\\mathcal{A}[S](x_0))^2]\\geq {\\mathbb{E}}_{Y_0}[(Y_0-{\\mathbb{E}}_{S}[\\mathcal{A}[S](x_0)])^2]$$ where $\\mathcal{A}[S]$ refers to the model obtained by calling $\\mathcal{A}$ with the training set $S$. The more the learning algorithm $\\mathcal{A}$ is unstable (high variance), the more the difference between these two quantities is large.\n",
    "\n",
    "The idea of the _bagging_ is to predict ${\\mathbb{E}}_{S}[\\mathcal{A}[S](x_0)]$ instead of predicting $\\mathcal{A}[S](x_0)$.  One issue is that we only have access to one training set $S$. To work around this problem, _bootstrap_ samples ${B_1}^{S},\\ldots,{B_b}^{S}$ of $S$ are used. Each _bootstrap_ sample ${B_i}^{S}$ can be regarded as a \"new\" training set. Then, using these ${B_i}^{S}$'s, ${\\mathbb{E}}_{S}[\\mathcal{A}[S](x_0)]$ is estimated by computing the mean of the $\\mathcal{A}[{B_i}^{S}](x_0)$.\n",
    "\n",
    "The _bootstrap_ is a resampling method. It draws a set $B$ from a set $S$ with $\\vert S\\vert=\\vert B\\vert=n$. The set $S$ is assumed to come from $n$ _i.i.d._ draws of a random variable $X$. We want to draw a new set $B$ but can not draw from the true $X$ anymore. Using the draws in $S$, the idea of the _bootstrap_ is to \"simulate\" the draw of $X$. A draw of $X$ will be \"simulated\" by drawing one element of $S$ with a uniform probability. The bootstrap sample $B$ is $n$ \"simulated\" draws.\n",
    "\n",
    "<font color='red'>**Q6:**</font> Write a function `bootstrap(X,y)` that returns a bootstrap sample of `X` and `y`.\n",
    "\n",
    "<font color='green'>**Hints for Q6:**</font> Using [`np.random.choice`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html) (click on it to see the documentation), one can compute a variable `indexes` containing the indexes for the selected examples. Then using this `indexes` one can create new numpy arrays using the syntaxe `array[indexes]` which create a new array containing the elements of `array` at the indexes `indexes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [],
   "source": [
    "def bootstrap(X,y):\n",
    "    n = X.shape[0]\n",
    "    sel = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "    return X[sel], y[sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Q7:**</font> Using `bootstrap` and `sklearn.tree.DecisionTreeRegressor`, write the function `bagging(X,y,ntrees,nmin)` that return a list of trees trained using a different bootstrap sample for each tree. Note that the default behavior of `sklearn.tree.DecisionTreeRegressor()` is to grow the tree till there are not enough examples to split (just like in <font color='red'>**Q4**</font>). Here `nmin` is the minimum number of samples in each leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "def bagging(X,y,ntrees,nmin):\n",
    "    trees = []\n",
    "    for _ in range(ntrees):\n",
    "        Xb, yb = bootstrap(X,y)\n",
    "        tree = sklearn.tree.DecisionTreeRegressor(min_samples_leaf=nmin, min_samples_split=2*nmin)\n",
    "        tree.fit(Xb,yb)\n",
    "        trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Q8:**</font> The prediction of a bagged models is the mean of the prediction of each model (a tree here). Write `mean_predict(trees, X)` that returns the mean prediction of the trees `trees` on the input `X`.\n",
    "\n",
    "<font color='green'>**Hints for Q8:**</font> One can initialize a variable `res=0`, and then loop on `trees` to sum the prediction of the tree and then divide the prediction by the number of trees at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [],
   "source": [
    "def mean_predict(trees,X):\n",
    "    res = 0\n",
    "    for tree in trees:\n",
    "        res += tree.predict(X)\n",
    "    return res/len(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Q9:**</font> Call the function `plotbagging`. This function trains 500 bootstrapped trees on the training set and then plots the performances on the validation sets as a function of the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predictions_from_trees(predict,trees, X):\n",
    "    return [predict(trees[:i],X) for i in range(1,len(trees)+1)]\n",
    "\n",
    "def rmses_from_predictions(predictions,y):\n",
    "    return [rmse(predi, y) for predi in predictions]\n",
    "\n",
    "def plotbagging():\n",
    "    X, y = load_data()\n",
    "    Xts, Xvs, yts, yvs = sklearn.model_selection.train_test_split(X, y, random_state=42,train_size =350)\n",
    "    ntrees = 500\n",
    "    nmin = 1\n",
    "    baggedtrees = bagging(Xts,yts,ntrees,nmin=nmin)\n",
    "    plt.plot(rmses_from_predictions(predictions_from_trees(mean_predict,baggedtrees,Xvs),yvs))\n",
    "    plt.show()\n",
    "\n",
    "# plotbagging() # uncomment and call this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotbagging() # uncomment and call this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "If we consider $X_1,\\ldots,X_b$ _i.i.d._ of variance $\\sigma^2$, we have ${\\mathbb{Var}}\\left[\\frac{1}{b}\\sum_{i=1}^b X_i\\right]=\\frac{\\sigma^2}{b}$. Using this fact, the bagging method averages different prediction to reduce the variance of the predicted value. This is done by predicting $\\frac{1}{b}\\sum_{i=1}^b \\mathcal{A}[{B_i}^{S}](x_0)$ instead of predicting $\\mathcal{A}[S](x_0)$. However, the variables $\\mathcal{A}[{B_i}^{S}](x_0)$ are not independent and they have a correlation coefficient $\\rho\\geq 0$. Thus, we have that ${\\mathbb{Var}}\\left[\\frac{1}{b}\\sum_{i=1}^b \\mathcal{A}[{B_i}^{S}](x_0)\\right]=\\rho \\sigma^2+\\frac{(1-\\rho)}{b}\\sigma^2$. \n",
    "\n",
    "Intuitively, the ${B_i}^{S}$ bootstrap samples are computed on the same $S$: if the drawn $S$ is different from the \"true\" distribution then all the bootstrap samples ${B_i}^{S}$ will be similar to $S$ and thus, they will be similarly different from the \"true\" distribution. \n",
    "\n",
    "The idea of the random forest is to reduce the correlation coefficient $\\rho$. To do this, the split selection is modified. If we have $p$ explanatory variables, instead of considering the $p$ variables for selecting the best split, we only consider $m$ variables. These $m$ variables are selected randomly among the $p$ variables at each split.\n",
    "\n",
    "<font color='red'>**Q10:**</font> When $m=p$, to which algorithm the random forest is equivalent ?\n",
    "\n",
    "<font color='red'>**Q11:**</font> When $m$ decreases, how the variance of the random forest algorithm vary ? Similarly, when $m$ decreases, how the bias of the random forest vary ?\n",
    "\n",
    "<font color='red'>**Q12:**</font> Write the function `random_forest(X, y, ntrees, m, nmin)` that returns a list of tree models. To grow these trees, each split variable will be selected among a random set of $m$ variables. This set is drawn each time a split has to be selected.\n",
    "\n",
    "<font color='green'>**Hints for Q12:**</font> To enforce that each split variable will be selected among a random set of $m$ variables, you can specify the $m$ value through the parameter `max_features` of the constructor `sklearn.tree.DecisionTreeRegressor`. The code of the `random_forest` function will be very similar to the `bagging` one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "source": [
    "<font color='blue'>**A10:**</font> Bagging.\n",
    "\n",
    "<font color='blue'>**A11:**</font> When m decreases the variance reduces (correlation between trees reduces), but the bias increase (the built trees are less \"optimal\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [],
   "source": [
    "def random_forest(X, y, ntrees, m,nmin):\n",
    "    trees = []\n",
    "    for _ in range(ntrees):\n",
    "        Xb, yb = bootstrap(X,y)\n",
    "        tree = sklearn.tree.DecisionTreeRegressor(max_features=m, min_samples_leaf=nmin, min_samples_split=2*nmin)\n",
    "        tree.fit(Xb,yb)\n",
    "        trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "_Boosting_ is a family of meta-algorithms. Considering a high bias-low variance learning algorithm $\\mathcal{A}$, a _boosting_ meta-algorithm learns a model by combining several models obtained by using $\\mathcal{A}$. As opposed to the _bagging_, the $\\mathcal{A}$ is applied to a different learning problem at each iteration.\n",
    "\n",
    "One of the first boosting algorithm is called _AdaBoost_ ([A decision-theoretic generalization of on-line learning and an application to boosting, Y. Freund and R. Schapire, Journal of computer and system sciences, 1997](https://www.cis.upenn.edu/~mkearns/teaching/COLT/adaboost.pdf)). It modifies the weight of each examples at each iteration. Another boosting algorithm, the _gradient boosting_ ([Greedy function approximation: a gradient boosting machine, J. Friedman, Annals of statistics, 2001](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)), changes the variable to be predicted at each iteration.\n",
    "\n",
    "Considering a quadratic loss function, the _gradient boosting_ algorithm consists in fitting the residuals at each iteration. The obtained sequence of models $h_1,\\ldots,h_n$ are combined to obtain $h$: $h=\\mu+\\nu \\sum_{i=1}^n h_i$ where $\\nu \\in \\left]0;1\\right]$ is the _shrinkage_ parameter and $\\mu$ is the mean of the variable to predict.\n",
    "\n",
    "\n",
    "<img src=\"algoboost.png\" width=\"500\">\n",
    "\n",
    "<font color='red'>**Q13:**</font> The constructor `sklearn.tree.DecisionTreeRegressor(max_depth)` grows a tree with a depth inferior to `max_depth`. This parameter is usually low, leading to a high bias-low variance learning algorithm. Using this constructor, write the function `gradient_boost(X, y, ntrees, nu, max_depth)` that returns a tuple containing $\\mu$ and the list of fitted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_boost(X, y, ntrees, nu, max_depth):\n",
    "    mu = np.mean(y)\n",
    "    residuals = y - mu\n",
    "    trees = []\n",
    "    for _ in range(ntrees):\n",
    "        tree = sklearn.tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "        tree.fit(X, residuals)\n",
    "        residuals = residuals - nu * tree.predict(X)\n",
    "        trees.append(tree)\n",
    "    return mu, trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Q14:**</font> Write the function `predict_from_gb(mu, nu, trees, X)` that returns the predictions of the gradient boosted model on the input `X`. To compute this prediction you will use the model returned by`gradient_boost`. This model is a tuple containing  $\\mu$ and the list of fitted trees. In `predict_from_gb`, these two values are two parameters: `mu` and `trees`.\n",
    "\n",
    "<font color='green'>**Hints for Q14:**</font> One can start by copy-paste the code in <font color='red'>**Q8**</font> as the obtained code should be quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "corrected"
    ]
   },
   "outputs": [],
   "source": [
    "def predict_from_gb(mu, nu, trees, X):\n",
    "    res = mu\n",
    "    for tree in trees:\n",
    "        res += nu * tree.predict(X)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Q15:**</font> Execute the code below to plot the RMSE on the validation as a function of the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.71688675880432\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "X, y = load_data()\n",
    "Xts, Xvs, yts, yvs = sklearn.model_selection.train_test_split(X, y, random_state=42,train_size =350)\n",
    "\n",
    "s=time.time()\n",
    "ntrees = 500\n",
    "nmin = 1\n",
    "nu = 0.05\n",
    "max_depth = 3\n",
    "m = 9\n",
    "\n",
    "baggedtrees = bagging(Xts,yts,ntrees,nmin=nmin)\n",
    "baggedrmses = rmses_from_predictions(predictions_from_trees(mean_predict,baggedtrees,Xvs),yvs)\n",
    "\n",
    "rftrees = random_forest(Xts,yts,ntrees,m=m,nmin=nmin)\n",
    "rfrmses = rmses_from_predictions(predictions_from_trees(mean_predict,rftrees,Xvs),yvs)\n",
    "\n",
    "mu, gbtrees = gradient_boost(Xts, yts, ntrees, nu, max_depth)\n",
    "def gbpredict(trees,X):\n",
    "    return predict_from_gb(mu,nu,trees,X)\n",
    "gbrmses = rmses_from_predictions(predictions_from_trees(gbpredict,gbtrees,Xvs),yvs)\n",
    "\n",
    "\n",
    "x = np.arange(1,ntrees+1)\n",
    "for y in [baggedrmses,rfrmses, gbrmses]:\n",
    "    plt.plot(x, y)\n",
    "plt.xlabel(\"Number of trees used\")\n",
    "plt.ylabel(\"RMSE on the validation set\")\n",
    "plt.legend(('Bagged Trees', 'Random Forest', 'Gradient Boosted Trees'), loc='upper right')\n",
    "plt.show()\n",
    "print(time.time()-s)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
